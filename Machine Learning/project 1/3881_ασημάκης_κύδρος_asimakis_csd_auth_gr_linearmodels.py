# -*- coding: utf-8 -*-
"""3881_Ασημάκης_Κύδρος_asimakis@csd.auth.gr_LinearModels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VeDy_kf7yqYBed3myIGIt-xFOh6WrT15

## About iPython Notebooks ##

iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. Make sure you fill in any place that says `# BEGIN CODE HERE #END CODE HERE`. After writing your code, you can run the cell by either pressing "SHIFT"+"ENTER" or by clicking on "Run" (denoted by a play symbol). Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\rightarrow$Run All).

 **What you need to remember:**

- Run your cells using SHIFT+ENTER (or "Run cell")
- Write code in the designated areas using Python 3 only
- Do not modify the code outside of the designated areas
- In some cases you will also need to explain the results. There will also be designated areas for that.

Fill in your **NAME** and **AEM** below:
"""

NAME = "Ασημάκης Κύδρος"
AEM = "3881"

"""---

# Assignment 1 - Linear Models #

Welcome to your first assignment. This exercise gives you a brief introduction to Python and the fundamental libraries for machine learning. It also gives you a wide understanding on how linear models work.

After this assignment you will:
- Be able to use iPython Notebooks
- Be able to use basic numpy and pandas functions
- Be able to build your first linear model from scratch
- Be able to use the basic functions of scikit-learn

## 1. Numpy & Pandas ##

The [**NumPy**](https://numpy.org/) library is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays.

The [**Pandas**](https://pandas.pydata.org/) library is built on NumPy and provides easy-to-use data structures and data analysis tools for the Python programming language.

Feel free to look at the documentation ([NumPy Doc](https://numpy.org/doc/1.18/user/quickstart.html) & [Pandas Doc](https://pandas.pydata.org/docs/)) of those libraries troughout this assignment.

As a convention we always import the libraries as follows:
"""

# Run this cell
import numpy as np
import pandas as pd

"""### 1.1 The very basic of NumPy ### (1 points)

**1.1.1 Exercise**: Create a 3-dimensional *NumPy* array (3x5) which contains the numbers 1-15 on variable 'a'. Moreover, create another *NumPy* array of dimensions (3x2) of random values (0 to 1) using the fucntions np.random.random on variable 'b'. *(An example of an expected outcome is also provided)*
"""

np.random.seed(42)
# BEGIN CODE HERE
a = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]])
b = np.array([[np.random.random() for _ in range(2)] for _ in range(3)])
# END CODE HERE
print(a)
print(b)

"""**1.1.2 Exercise**: Perform the dot product of 'a' transpose with 'b' and assign the result to variable 'c' using Numpy. *(An example of an expected outcome is also provided, the produced matrix should have 5x2 dimensions)*"""

# BEGIN CODE HERE
c = np.dot(a.T, b)
# END CODE HERE
print(c)

"""Two common numpy functions used are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html).
- X.shape is used to get the shape (dimension) of a matrix/vector X.
- X.reshape(...) is used to reshape X into some other dimension.

**1.1.3 Exercise**: Use the function **shape** to print the shape of variable **c** and the function **reshape** to change the dimensions of **c** from 5x2 to 1x10. *(An example of an expected outcome is also provided)*
"""

np.random.seed(42)
# BEGIN CODE HERE
shape_c =  c.shape
reshaped_c = c.reshape((1, 10))
#END CODE HERE
print("shape_random_array:", shape_c)
print("reshaped_one_array:", reshaped_c)

"""**1.1.4 Exercise**: Create an array of 9 evenly spaced values from 0 to 5 using the **np.linspace()** function. (An example of an expected outcome is also provided)"""

# BEGIN CODE HERE
x = np.linspace(0, 5, 9)

#END CODE HERE
print("x: " + str(x))

"""**1.1.5 Exercise**: Create a dictionary (hashmap) containing two keys: *lin_space*, *dot_product* and assign them the variables **x**, and **c**, respectively. Then, you should retrieve the values of both keys and print them. (An example of an expected outcome is also provided)"""

# BEGIN CODE HERE
hashmap = dict({"lin_space"     :  x,
                             "dot_product" :  c})
print(hashmap["lin_space"])
print(hashmap["dot_product"])
#END CODE HERE
assert (isinstance(hashmap, dict))

"""### 1.2 The very basics of Pandas ### (0.5)

- Read & Write CSV
- slices

**1.2.1 Exercise:** Read the file input.csv into a dataframe using the pandas read_csv() function. (An example of an expected outcome is also provided)
"""

# BEGIN CODE HERE
from google.colab import files
df = pd.read_csv("input.csv")

#END CODE HERE
df.head()

"""**1.2.2 Exercise:** Find the mean and std value of the *thalach* variable using **pandas**. (An example of an expected outcome is also provided)

"""

# BEGIN CODE HERE
mean_thalach = df['thalach'].mean()
std_thalach = df['thalach'].std()
#END CODE HERE
print("Average: " + str(mean_thalach))
print("STD: " + str(std_thalach))

"""**1.2.3 Exercise:** Select the rows where *thalach*>150 and only the columns age, sex, cp, chol, target using **pandas**. (An example of an expected outcome is also provided)"""

# BEGIN CODE HERE
selected = df[['age', 'sex', 'cp', 'chol', 'target']].loc[df['thalach'] > 150]
#END CODE HERE
selected

"""## 2.0 Linear Models ##

In this part of the excersice you are going to build a logistic regression model from scratch.

**2.1 Exercise:** Implement and test the sigmoid function using numpy. (An example of an expected outcome is also provided)

sigmoid function:
$$\sigma(t)= \dfrac{1}{1 + exp(-t)}$$
"""

from numpy import exp
def sigmoid(t: np.array):
    """
    Compute the sigmoid of t
    Arguments:
    t -- A numpy array of any size

    Return:
    s -- sigmoid(t)
    """
    # BEGIN CODE HERE
    return 1 / (1 + exp(-t))
    #END CODE HERE

x = np.array([7, 1, 9])
# BEGIN CODE HERE
# Call the function given the variable x and print it
print(sigmoid(x))
#END CODE HERE

"""**2.2 Exercise**: Implement parameter initialization in the cell below. You have to initialize w (weight vector) and b (bias) with zeros. Then, run the function using the **dim** variable. *(An example of an expected outcome is also provided)*"""

def initialize(__dim: int):
    """
    Argument:
    dim -- the number of parameters

    Returns:
    w -- initialized vector of shape (1, dim)
    b -- initialized bias weight
    """
    # BEGIN CODE HERE
    return np.array([0.0 for _ in range(__dim)]), 0.0
    #END CODE HERE
dim = 7
# BEGIN CODE HERE
w, b = initialize(dim)
#END CODE HERE
print ("w = " + str(w))
print ("b = " + str(b))

"""**2.3 Exercise**: Compute the cost of logistic regression using the sigmoid function above. You can find the dot product of two arrays by using the [np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html). Check slide 39 (41). (Tip 1: you should append b (weight of bias) in front of w array, Tip 2: Pay attention to 'ML Course - Linear Models' covered in class to see how to combine $x_0$ with the other variables of $X$. Tip 3: You may use np.array, np.c_, np.dot, np.log, np.sum). Finally, run the function and check the results. *(An example of an expected outcome is also provided)*"""

def compute_cost(w,b,X,Y):
    """
    Arguments:
    w -- weights
    b -- bias
    X -- input data
    Y -- target or label vector

    Return:
    sigma -- the sigmoid of the z
    cost -- cost for logistic regression
    """
    # BEGIN CODE HERE
    weights = np.array([b] + list(w))
    x = np.array([[1] + list(xi) for xi in X])
    h = sigmoid(np.dot(x, weights.T))
    cost =np.sum((Y*np.log(h) + (1-Y)*np.log(1-h)))
    return h, (-1)*cost/len(Y)
    #END CODE HERE

w, b, X, Y = np.array([0.,1.]), 7., np.array([[0.,2.],[3.,4.],[-8.,-3.2]]), np.array([0,0,1])
# BEGIN CODE HERE
sigma, cost = compute_cost(w, b, X, Y)
#END CODE HERE
print("Sigmoid:",[i for i in sigma])
print("Cost:", str(cost))

"""**2.4 Exercise** Compute the gradient of w and b. Compute grad as in slide 40 (Compute X and bw like above). Then. run the function with the provided example. *(An example of an expected outcome is also provided)*"""

def gradient(w: np.array, b: float, X: np.array, Y: np.array, h: np.array):
    """
    Arguments:
    w -- weights
    b -- bias
    X -- input data
    Y -- target or label vector

    Return:
    dw -- gradient of the loss with respect to w (numpy array)
    db -- gradient of the loss with respect to b (scalar)
    """
    # BEGIN CODE HERE
    weights = np.array([b] + list(w))
    x = np.array([[1] + list(xi) for xi in X])
    anadelta = np.dot(x.T, (h - Y)) / len(Y)  # h is returned from the previous function
    return anadelta[1:], anadelta[0]
    #END CODE HERE

w, b, X, Y = np.array([0.,1.]), 7., np.array([[0.,2.],[3.,4.],[-8.,-3.2]]), np.array([0,0,1])
# BEGIN CODE HERE
dw, db = gradient(w, b, X, Y, sigma)
#END CODE HERE
print ("dw = " + str(dw))
print ("db = " + str(db))

"""**2.5 Exercise** Implement the parameters update function below. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\theta$, the update rule is $ \theta = \theta - \alpha \text{ } d\theta$, where $\alpha$ is the learning rate. Tip: Use the functions developed above. Then, run the function on the provided example and check the results. *(An example of an expected outcome is also provided)*"""

def update_parameters(w: np.array, b: float, X: np.array, Y: np.array, num_iterations: int, learning_rate: float):
    """
    This function optimizes w and b by running a gradient descent algorithm

      Arguments:
      w -- weights
      b -- bias
      X -- input data
      Y -- target or label vector
      num_iterations -- number of iterations of the optimization loop
      learning_rate -- learning rate of the gradient descent update rule

      Returns:
      params -- dictionary containing the weights w and bias b
      grads -- dictionary containing the gradients of the weights and bias with respect to the cost function.
    """
    for i in range(num_iterations):

        # BEGIN CODE HERE

        # Cost and gradient calculation
        h, cost = compute_cost(w, b, X, Y)
        dw, db = gradient(w, b, X, Y, h)
        # Update rule
        b -= learning_rate*db
        w -= learning_rate*dw
        # END CODE HERE

        # Print the cost every 100 training iterations
        if i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return w, b, dw, db

w, b, X, Y = np.array([0.,1.]), 7., np.array([[0.,2.],[3.,4.],[-8.,-3.2]]), np.array([0,0,1])
num_iterations= 100
learning_rate = 0.009

# BEGIN CODE HERE
w, b, dw, db = update_parameters(w, b, X, Y, num_iterations, learning_rate)
# END CODE HERE
print ("w = " + str(w))
print ("b = " + str(b))
print ("dw = " + str(dw))
print ("db = " + str(db))

"""**2.6 Predict** Implement the predict() function by calculating the $y'$ and then convert the probabilities to actual predictions 0 or 1. Finally, run the function with the provided example. *(An example of an expected outcome is also provided)*"""

def predict(w: np.array, b: float, X: np.array):
    '''
    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)

    Arguments:
    w -- weights
    b -- bias, a scalar
    X -- input data

    Returns:
    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
    '''

    # BEGIN CODE HERE
    weights = np.array([b] + list(w))
    x = np.array([[1] + list(xi) for xi in X])
    return np.array([0 if s < 0.5 else 1 for s in sigmoid(np.dot(x, weights.T))])
    # END CODE HERE

w = np.array([0.1124579,0.23106775])
b = 0.2
X = np.array([[0.,0.2],[-2.1,1.],[-2.2,-1.1]])
prediction = predict(w, b, X)
# BEGIN CODE HERE
#prediction = ...
# END CODE HERE
print("predictions = " + str(prediction))

"""**2.7 Exercise** Put all the above blocks in the right order to create a model in the function below."""

def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5):
    """
    Builds the logistic regression model by calling the function you've implemented previously

    Arguments:
    X_train -- training set represented by a numpy array
    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
    X_test -- test set represented by a numpy array of shape
    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
    learning_rate -- hyperparameter representing the learning rate used in the update rule of update_parameters()

    Returns:
    d -- dictionary containing information about the model.
    """

    # BEGIN CODE HERE

    # initialize parameters
    w, b = initialize(len(X_train[0]))

    # Gradient descent
    w, b, dw, db = update_parameters(w, b, X_train, Y_train, num_iterations, learning_rate)

    # Predict test/train set examples
    y_pred_test = predict(w, b, X_test)
    y_pred_train = predict(w, b, X_train)

    #END CODE HERE


    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))


    d = {"Y_prediction_test": y_pred_test,
        "Y_prediction_train" : y_pred_train,
        "w" : w,
        "b" : b,
        "learning_rate" : learning_rate,
        "num_iterations": num_iterations}

    return d

"""**2.8 Exercise** Using the **df** from above assign to the **X** variable the values of the columns 'oldpeak' and 'thalach', and in the **y** variable the column 'target'. *(An example of an expected outcome (the plot) is also provided)*"""

import matplotlib.pyplot as plt

# BEGIN CODE HERE

X = df[['oldpeak', 'thalach']].values
y = df['target'].values
#END CODE HERE


plt.figure(figsize=(8,8))
colors = ['blue' if i == 0 else 'red' for i in y]
plt.scatter(X[:, 0],
            X[:, 1],
            alpha = .4, color=colors)

"""**2.9 Exercise** Split your dataset into train and test set and then use the model function to evaluate your model. Be careful to include both classes in train and test set. Finally, make a plot containing the samples and the line the model has learned. Use train_test_split from sklearn to split X and y to train and test sets of 50% and 50%, respectively. You should also use the shuffle parameter. [Train-test-split Documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) *(An example of an expected outcome is also provided)*"""

# BEGIN CODE HERE
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.5, shuffle = True, random_state = 77)
#END CODE HERE

d = model(X_train,y_train,X_test,y_test,num_iterations = 1000, learning_rate = 0.0001)

# Plot again
plt.figure(figsize=(8,8))
colors = ['blue' if i == 0 else 'red' for i in y_train]
plt.scatter(X_train[:, 0], X_train[:, 1], alpha = .4, color=colors)

x_boundary = np.linspace(0, 4, 1000) # Return evenly spaced numbers over a specified interval.
weights =  d['w'][0]

# BEGIN CODE HERE
# line: theta_2*y + theta_1*x + theta_0*1 = 0 =>
# y = -(theta_1*x + theta_0) / theta_2
y_boundary = (-1)*(d['b'] + x_boundary * weights) / d['w'][1]
#END CODE HERE

plt.plot(x_boundary, y_boundary, color='black')

"""**2.10 Bonus Exercise:** This exercise is optional, but if you implement it you will receive a bonus. Moreover, it will make the following exercise much easier (+ it will help you in the future in general). Let's create a class containing all the neccessary functions to train a Logistic Regression model and make predictions.

**self** represents the instance of the class. By using the **self** keyword we can access the attributes (public variables) and methods (functions) of the class in python [Source](https://www.geeksforgeeks.org/self-in-python-class/). It binds the attributes with the given arguments of an instance. Therefore, for the class MyLogisticRegression, we can create an instance of this class:

```
lr = MyLogisticRegression(...)
```
And we can use its public variables and methods
```
lr.num_iterations = 2000
print(lr.num_iterations) #Will print the public variable
```
Hint: Inside the class, any shared public variable x should be accessed through 'self.x'.
```
def inner_function (self, y):
  print(self.x, y)
```
"""

from numpy import exp

class MyLogisticRegression:
    def __init__(self, num_iterations=2000, learning_rate=0.004):
          self.w = None
          self.b = None
          self.num_iterations = num_iterations
          self.learning_rate = learning_rate

    def _prepare_vectors(self, X): # mine
          weights = [self.b] + list(self.w)
          x = [[1] + list(xi) for xi in X]
          return np.array(weights), np.array(x)

    def _print_accuracy(self, Y_train, Y_test, Y_pred_train, Y_pred_test, decimals): # mine
          train_percentage = 100 - np.mean(np.abs(Y_pred_train - Y_train)) * 100
          test_percentage  = 100 - np.mean(np.abs(Y_pred_test - Y_test)) * 100
          print("train accuracy: {:.{}f} %".format(train_percentage, decimals))
          print("test accuracy: {:.{}f} %".format(test_percentage, decimals))

    def _sigmoid(self, t):
          return  1 / (1 + exp(-t))

    def _initialize(self, dim):
          self.w, self.b = np.array([0.0 for _ in range(dim)]), 0.0

    def _compute_cost(self, X, Y):
          weights, x = self._prepare_vectors(X)
          h = self._sigmoid(np.dot(x, weights.T))
          cost = (-1) * sum((Y * np.log(h) + (1 - Y) * np.log(1 - h))) / len(Y)
          return h, cost

    def _gradient(self, X, Y, sigma):
          weights, x = self._prepare_vectors(X)
          __gradient = np.dot(x.T, (sigma - Y)) / len(Y)
          return __gradient[1:], __gradient[0]

    def _update_parameters(self, X, Y):
          costs = {}
          for i in range(self.num_iterations):
              h, cost = self._compute_cost(X, Y)
              dw, db = self._gradient( X, Y, h)
              self.w -= self.learning_rate * dw
              self.b -= self.learning_rate * db
              # return the costs for each epoch in a dictionary so it doesn't spam the feed
              if i % 100 == 0:
                  costs[i] = cost
          return dw, db, costs

    def _predict(self, X):
          weights, x = self._prepare_vectors(X)
          predictions = [0 if s < 0.5 else 1 for s in self._sigmoid(np.dot(x, weights.T))]
          return np.array(predictions)

    def fit_evaluate(self, X_train, Y_train, X_test, Y_test):
          self._initialize(len(X_train[0]))
          self._update_parameters(X_train, Y_train)
          y_pred_test, y_pred_train = self._predict(X_test), self._predict(X_train)
          # Print train/test Errors
          self._print_accuracy(Y_train, Y_test, y_pred_train, y_pred_test, 2)
          return  {"Y_prediction_test": y_pred_test,  "Y_prediction_train" : y_pred_train, "w" : self.w,
                       "b" : self.b, "learning_rate" : self.learning_rate, "num_iterations": self.num_iterations}

linearModel = MyLogisticRegression(1000, 0.0001)
d = linearModel.fit_evaluate(X_train, y_train, X_test, y_test)

"""## 3.0 Regularization ## (2 points)

Rewrite any of the above functions in the below block so as the Logistic Regression to have the option to run with **L1** and **L2** regularization. Rewrite only the functions needed. (Tip: If you use the class, this excercise is much easier. You should rewrite the class, changing the constructor of the class and only one more function to implement the regularization. Then, you will be able to call those commands for the different regularization parameters)

```
linearModel = MyLogisticRegression(1000, 0.0001, 'l1')
linearModel = MyLogisticRegression(1000, 0.0001, 'l2')
```

"""

### BEGIN CODE HERE
class RegularizedLR(MyLogisticRegression):
        def __init__(self, num_iterations, learning_rate, regularization, hyperparameter):
              super().__init__(num_iterations, learning_rate)
              self.regularization = regularization
              self.hyperparameter = hyperparameter

        def _update_parameters(self, X, Y):
            costs = {}
            for i in range(self.num_iterations):
                h, cost = self._compute_cost(X, Y)
                dw, db = self._gradient( X, Y, h)
                # compute regularization elements
                alpha = self.hyperparameter / len(Y)
                weights = np.array([self.b] + list(self.w))
                if self.regularization == 'L2':
                      R = sum([weight * weight for weight in weights]) * alpha / 2
                      dR = alpha * weights
                      cost += R
                      db += dR[0]
                      dw += dR[1:]
                if self.regularization == 'L1':
                      R = sum([np.abs(weight) for weight in weights]) * alpha
                      dR = alpha * np.sign(weights)
                      cost += R
                      db += dR[0]
                      dw += dR[1:]
                self.w -= self.learning_rate * dw
                self.b -= self.learning_rate * db
                # return the costs for each epoch in a dictionary so it doesn't spam the feed
                if i % 100 == 0:
                    costs[i] = cost
            return dw, db, costs

#END CODE HERE

#You can freely test your code here
linearModel0 = MyLogisticRegression(1000, 0.0001)
linearModel1 = RegularizedLR(1000, 0.0001, 'L1', 1)
linearModel2 = RegularizedLR(1000, 0.0001, 'L2', 1)
print('Results for model 0:')
d0 = linearModel0.fit_evaluate(X_train, y_train, X_test, y_test)
print('\nResults for model 1:')
d1 = linearModel1.fit_evaluate(X_train, y_train, X_test, y_test)
print('\nResults for model 2:')
d2 = linearModel2.fit_evaluate(X_train, y_train, X_test, y_test)

"""## **Questions** ## (1 point)

Answer below!
1. What is the purpose of splitting the data into training and testing sets? [0.1]
2. What is the role of the intercept in linear regression? Why is it important? [0.1]
3. Your model should achieve around 70% accuracy in the test set. If you want to improve the accuracy what changes you should make? Report the changes and the results. [0.2]
4. Besides using a specific number of iterations for your model what else you can do to stop the training? [0.2]
5. Do you notice any differences when using the L1 or L2 regularization? Is so, why? If not, why? (Answer based on what you've learned in class) [0.2]
6. What are some common applications of linear models in real-world scenarios? How do these models compare to other machine learning algorithms in terms of performance, and interpretability? [0.2]

Bonus Question:
*What parts of this assignment were not clear or misleading? Are there any other comments on this assignment?*
"""

#YOUR ANSWER HERE
pass

"""1. If we trained the model with all the data it could overfit on the dataframe. We use a portion of it as tests so it can learn general patterns and not just satisfy the training set.
2. The bias is used as a way for the output line of the regression to be free in the plane. If it didn't exist then every line would have to pass from the origin. It's important because it allows the model to be more precise with its decision boundary.
3.  Feed it more/more diverse data and/or use regularization. Both aim to
smooth the dataframe and thus nudge the model into more general patterns.
4.  Early stopping, regularization or manually select subsets of the available features.
5.  Using L1, the model scores worse in the training. L1 regularization nullifies the coefficients that are of insignificant value in order to combat
overfitting and thus discover more general patterns.
6.  Business Intelligence uses them heavily for knowledge discovery. Other algorithms, such as decision trees or neural networks, might not be as interpretable or fast as a linear model, but they are vastly more performant.

7.
    i) I never got to see the "expected output" in any task.

    ii)  Many headers were misleading. Particularly in 3.0, I interpret it as "make it so the class MyLogisticRegression can be called with parameters 1000, 0.0001, 'l1'" while also being expected to use inheritance. Moreover,  the hyperparameter isn't mentioned anywhere whilst being essential in building the regression.

    iii) There were instances where I had to debug code already given to me.

    iv) I don't get the point of using a python notebooks environment for a "fill the boxes" exercise.

    v) I would much rather this last 'bonus question' about complaints be anonymous.
"""