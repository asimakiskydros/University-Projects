# Neural Networks - Deep Learning

Each different NN architecture had its own project:

0. Commit to a big, real-world-data dataset for the rest of your projects.
   Try to predict it with 1-Nearest Neighbor, 3-Nearest Neighbors and Nearest Centroid classifiers. Record the results.
   (*Basically a nothingburger project to get us used to dealing with large scale data and underlining that such problems are overcomplex for simple Machine Learning algorithms.*)

1. Use a Multilayer Perceptron to predict the dataset chosen at project #0. 
   Implement your own or pick a pre-existing one. Provide train-test scores, correct and wrong classifications, test different
   hyperparameter combinations. Compare its performance with that of the classifiers of project #0.
   
2. Use a Support Vector Machine to predict 2 or all the classes from the chosen dataset.
   Implement your own or pick a pre-existing one (in any case, use Quadratic Programming). Provide train-test scores, correct
   and wrong classifications, test different hyperparameter combinations. Attempt to append an SVM to an MLP. Record the results.
   Compare the SVM's performance to that of the classifiers of project #0.

3. Use a Radial Basis Function NN OR a Hebbian Learning Network OR an Autoencoder to predict
   the chosen dataset or, in the case of autoencoders, to reconstruct the same or some different dataset like MNIST.
   Use the Autoencoder to reconstruct the next digit from the one given as input. Alternatively/Moreover, implement
   an adder, such that the autoencoder reconstructs a pair of images representing the sum of the pair of images given
   as input. Compare its performance with image reconstruction through PCA. Provide train-test scores, test different
   hyperparameter combinations.

   *The original headers in greek are provided in each corresponding directory apart from the first (project #0). Courtesy of Aristotle University of Thessalonica, CSD*
   
